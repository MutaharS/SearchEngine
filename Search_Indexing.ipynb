{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Search Indexing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rl7zLCx1WaPk",
        "outputId": "d299ded9-a45e-4ce1-ccee-168a317f4a5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 29 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 40.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=c0a23a6cc5e0ee6676e6f42955c0de809dbab9f7a0532cb823806d133a5a829f\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Give Colab Access to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHaA2xb7W5Fd",
        "outputId": "1f723a1f-5870-44f6-cbed-71561c4e3971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('drive/MyDrive/SearchEngine')"
      ],
      "metadata": {
        "id": "OFDeMgzEYRB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "N-Gram Processing\n",
        "\n",
        "Not only word counts, but N-Gram Counts\n",
        "\"\"\"\n",
        "N = 2\n",
        "class NGramException(Exception):\n",
        "  pass\n",
        "\n",
        "def N_2_Gram(content, N=2):\n",
        "  words = content.lower().split(' ')\n",
        "  if N > len(words):\n",
        "    raise NGramException('Trying to split into N-Grams on content with less than N words.')\n",
        "  grams = []\n",
        "  for i in range(len(words) - N + 1):\n",
        "    grams.append(' '.join(words[i : i + N]))\n",
        "  return grams\n",
        "\n",
        "def N_3_Gram(content, N=3):\n",
        "  words = content.lower().split(' ')\n",
        "  if N > len(words):\n",
        "    raise NGramException('Trying to split into N-Grams on content with less than N words.')\n",
        "  grams = []\n",
        "  for i in range(len(words) - N + 1):\n",
        "    grams.append(' '.join(words[i : i + N]))\n",
        "  return grams\n",
        "\n",
        "# content_map = {\n",
        "#   'link1': 'q',\n",
        "#   'link2': 'this is. like. another sentence or two'\n",
        "# }\n",
        "\n",
        "# content_map = {k : content_map[k] for k in content_map.keys() if len(content_map[k]) != 0 and len(content_map[k].split(' ')) > N}\n",
        "\n",
        "# # Get (link, content) for each link\n",
        "# raw_in = sc.parallelize([(l,c) for l,c in content_map.items()])\n",
        "\n",
        "# # def get_lookup_for_single_words(N=1):\n",
        "\n",
        "# if N == 1:\n",
        "#   # Get (link, word) for each row\n",
        "#   # We do this by splitting the text content and flattening on the values\n",
        "#   tf_map_input = raw_in.flatMapValues(lambda content: content.lower().strip().split(' '))\n",
        "# elif N == 2:\n",
        "#   # Get (link, 2-grams) for each row\n",
        "#   tf_map_input = raw_in.flatMapValues(NGram)\n",
        "\n",
        "# tf_map_input.collect()"
      ],
      "metadata": {
        "id": "ntqfGoPVFXvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Script\n",
        "from pyspark.context import SparkContext\n",
        "from typing import List, Tuple\n",
        "import json, math\n",
        "import numpy as np\n",
        "\n",
        "# sc = SparkContext('local', 'term frequency')\n",
        "with open('json_files/content.json', 'r') as content_json:\n",
        "\tcontent_map = json.load(content_json)\n",
        " \n",
        "# Clean empty content, and have at least 3 words per content\n",
        "content_map = {k : content_map[k] for k in content_map.keys() if len(content_map[k]) != 0 and len(content_map[k].split(' ')) > N}\n",
        "\n",
        "\"\"\"\n",
        "Calculate the Term Frequency\n",
        "Raw Input: (link, content)\n",
        "Mapper Input: (link, word)\n",
        "Mapper Output: ((link, word), 1)\n",
        "Reducer: ((link, word), TF)\n",
        "\"\"\"\n",
        "# Get (link, content) for each link\n",
        "raw_in = sc.parallelize([(l,c) for l,c in content_map.items()])\n",
        "\n",
        "def get_lookup(N=1):\n",
        "\n",
        "  if N == 1:\n",
        "    # Get (link, word) for each row\n",
        "    # We do this by splitting the text content and flattening on the values\n",
        "    tf_map_input = raw_in.flatMapValues(lambda content: content.lower().strip().split(' '))\n",
        "  elif N == 2:\n",
        "    # Get (link, 2-grams) for each row\n",
        "    print('Calling flatMapValues(NGram)')\n",
        "    tf_map_input = raw_in.flatMapValues(N_2_Gram)\n",
        "  elif N == 3:\n",
        "    # Get (link, 3-grams) for each row\n",
        "    tf_map_input = raw_in.flatMapValues(N_3_Gram)\n",
        "\n",
        "  # print('Filtering out empty words')\n",
        "  # Filter out empty words\n",
        "  # tf_map_input_cleaned = tf_map_input.filter(lambda tup: tup[0][1] != '')\n",
        "\n",
        "  print('Getting mapper output ((link, word), 1)')\n",
        "  # Get ((link, word), 1) for each row\n",
        "  tf_map_output = tf_map_input.map(lambda x: (x,1))\n",
        "\n",
        "  print('Reducing to ((link, word), TF)')\n",
        "  # Reduce on the key and sum up the ones\n",
        "  # We now have ((link, word), TF)\n",
        "  tf_red_output = tf_map_output.reduceByKey(lambda a,b: a + b)\n",
        "\n",
        "  \"\"\"\n",
        "  Calculate the Document Frequency\n",
        "  Mapper Input: ((link, word), TF)\n",
        "  Mapper Output: (word, 1)\n",
        "  Reducer Output: (word, DF)\n",
        "\n",
        "  Mapper Input: ((link, word), TF)\n",
        "  Mapper Output: ((word, link), (TF, DF))\n",
        "  \"\"\"\n",
        "\n",
        "  print('Mapping (word, 1)')\n",
        "  # Find the document frequency\n",
        "  # select distinct words in link_to_word_count\n",
        "  df_map_output = tf_red_output.map(lambda tup: (tup[0][1], 1))\n",
        "\n",
        "  print('Reducing to (word, DF)')\n",
        "  # Sum 1s, to get DF\n",
        "  df_red_output = df_map_output.reduceByKey(lambda a,b: a + b)\n",
        "\n",
        "  # Store the DF of each word in a dict\n",
        "  word_to_df = {}\n",
        "  \n",
        "  print('Calling df_red_output.collect()')\n",
        "  document_frequencies = df_red_output.collect()\n",
        "\n",
        "  print('Storing word -> DF into dict')\n",
        "  for word,freq in document_frequencies:\n",
        "    word_to_df[word] = freq\n",
        "\n",
        "  # Add the DF to each record\n",
        "  tf_df_rdd = tf_red_output.map(lambda tup: [tup[0][1], tup[0][0], tup[1], word_to_df[tup[0][1]]])\n",
        "\n",
        "  \"\"\"\n",
        "  Calculate the TF-IDF of word per link (relevance of that word in the body of text for that link)\n",
        "\n",
        "  TF = Word Frequency in that document\n",
        "  IDF = log(# of Documents / Document Frequency)\n",
        "  TF-IDF = TF * IDF\n",
        "\n",
        "  Mapper Input: [word, link, TF, DF]\n",
        "  Mapper Output: (word, document, TF-IDF)\n",
        "  \"\"\"\n",
        "\n",
        "  # Calculate the total number of documents\n",
        "  total_documents = len(content_map)\n",
        "\n",
        "  # Function for calculating & saving the TF-IDF\n",
        "  def calculate_TF_IDF(record):\n",
        "    # input: [word, link, TF, DF]\n",
        "    # output: (word, link, TF-IDF)\n",
        "    # Calculate idf\n",
        "    idf = np.log(total_documents / int(record[3]))\n",
        "    \n",
        "    # Calculate tf-idf\n",
        "    tf_idf = int(record[2]) * idf\n",
        "\n",
        "    return (record[0], [(record[1], tf_idf)])\n",
        "\n",
        "  # Calculate the TF-IDF for each word in links\n",
        "  tf_idf_rdd = tf_df_rdd.map(calculate_TF_IDF)\n",
        "\n",
        "  \"\"\"\n",
        "  For searching, we want to speed up search. We'll have an RDD that is\n",
        "  mimicking a lookup table (map) that has keys as individual words. \n",
        "  The value will be a list of links, sorted by the TF-IDF,\n",
        "  that is to say, the links in which the word is most semantically relevant will be prioritized.\n",
        "  We can output this as a JSON of the format:\n",
        "  {\n",
        "    \"word_i\" : [link_0, link_1, ..., link_m],\n",
        "    ...\n",
        "    \"word_n\" : [link_0, link_1, ..., link_m],\n",
        "  }\n",
        "  \"\"\"\n",
        "  # Function that reduces the individual links (into one large list of links)\n",
        "  def aggregateLinks(v1, v2):\n",
        "    if v2 and len(v1) < 100:\n",
        "      v1.extend(v2)\n",
        "    return v1\n",
        "\n",
        "  # Apply the function to get the word -> [(link_0, tf-idf), ..., (link_n, tf-idf)] RDD\n",
        "  grouped_words = tf_idf_rdd.reduceByKey(aggregateLinks)\n",
        "\n",
        "  # Sort the list of links by tf-idf (desc)\n",
        "  def sort_links(record: List[Tuple]):\n",
        "    return (record[0], sorted(record[1], key=lambda value: value[1], reverse=True))\n",
        "\n",
        "  # Now we have a lookup table for searching ready!!\t\n",
        "  lookup_table = grouped_words.map(sort_links) # (word, [(link, TF-IDF), (link, TF-IDF)]\n",
        "\n",
        "  return lookup_table"
      ],
      "metadata": {
        "id": "48OyNlgAW-22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lookup_1gram = get_lookup(N=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_H1W90CJLhYV",
        "outputId": "bfbe8d1c-10fa-47f7-9095-250d9b57ca40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting mapper output ((link, word), 1)\n",
            "Reducing to ((link, word), TF)\n",
            "Mapping (word, 1)\n",
            "Reducing to (word, DF)\n",
            "Calling df_red_output.collect()\n",
            "Storing word -> DF into dict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect() too much memory usage, save it as text file and read it back line by line\n",
        "lookup_1gram.saveAsTextFile('1gram_lookup_2')"
      ],
      "metadata": {
        "id": "hKVqSljbXnAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lookup_2gram = get_lookup(N=2)"
      ],
      "metadata": {
        "id": "OtDkDAoRLUJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c65636f-de63-4776-ad94-117b91a578a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling flatMapValues(NGram)\n",
            "Getting mapper output ((link, word), 1)\n",
            "Reducing to ((link, word), TF)\n",
            "Mapping (word, 1)\n",
            "Reducing to (word, DF)\n",
            "Calling df_red_output.collect()\n",
            "Storing word -> DF into dict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect() too much memory usage, save it as text file and read it back line by line\n",
        "lookup_2gram.saveAsTextFile('2gram_lookup_2')"
      ],
      "metadata": {
        "id": "aCm23128cmGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lookup_3gram = get_lookup(N=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqVv0lcwO1jx",
        "outputId": "1ad90a23-51a2-493c-d603-3b639bd1dc3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting mapper output ((link, word), 1)\n",
            "Reducing to ((link, word), TF)\n",
            "Mapping (word, 1)\n",
            "Reducing to (word, DF)\n",
            "Calling df_red_output.collect()\n",
            "Storing word -> DF into dict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect() too much memory usage, save it as text file and read it back line by line\n",
        "lookup_3gram.saveAsTextFile('3gram_lookup')"
      ],
      "metadata": {
        "id": "TEt6zDhhSYBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Match this regular expression to find words / links\n",
        "word_pattern = r'\\'\\s*[a-zA-Z0-9:\\./!@#$%\\^&\\*\\(\\)\\-_+=~`<>,\\.\\?/]+\\s*\\''\n",
        "word_pattern = r'\\'[a-zA-Z0-9:\\./!@#$%\\^&\\*\\(\\)\\-_+=~`<>,\\.\\?/\\s]+\\''\n",
        "\n",
        "def get_map(nGram=1):\n",
        "  if nGram == 1:\n",
        "    file_path = '1gram_lookup_2/part-00000'\n",
        "    word_pattern = r'\\'\\s*[a-zA-Z0-9:\\./!@#$%\\^&\\*\\(\\)\\-_+=~`<>,\\.\\?/]+\\s*\\''\n",
        "  elif nGram >= 2:\n",
        "    file_path = '2gram_lookup_2/part-00000'\n",
        "    word_pattern = r'\\'[a-zA-Z0-9:\\./!@#$%\\^&\\*\\(\\)\\-_+=~`<>,\\.\\?/\\s]+\\''\n",
        "  else:\n",
        "    print('Invalid nGram Value.')\n",
        "    return {}\n",
        "  link_pattern = r'\\(\\'[a-zA-Z0-9:\\./!@#$%\\^&\\*\\(\\)\\-_+=~`<>\\.\\?/]+\\',\\s*[\\d\\.]+\\)'\n",
        "\n",
        "  def extract_word(line):\n",
        "    prog = re.compile(word_pattern, flags=re.ASCII)\n",
        "    match = prog.search(line)\n",
        "    if match:\n",
        "      return match.group(0)[1:-1]\n",
        "    return None\n",
        "\n",
        "  def extract_links(line):\n",
        "    prog = re.compile(link_pattern)\n",
        "    matches = prog.findall(line)\n",
        "    if matches:\n",
        "      return matches\n",
        "    return None\n",
        "\n",
        "  lookup = {}\n",
        "  with open(file_path, 'r') as lookup_file:\n",
        "    for line in lookup_file:\n",
        "      word = extract_word(line)\n",
        "      if not word:\n",
        "        continue\n",
        "      if 'http' in word:\n",
        "        continue\n",
        "      \n",
        "      line = line[2 + len(word) + 3 : ]\n",
        "      # print(line)\n",
        "      links = extract_links(line)\n",
        "      # print(links)\n",
        "      if not links:\n",
        "        continue\n",
        "      \n",
        "      lookup[word] = links\n",
        "  return lookup"
      ],
      "metadata": {
        "id": "0U9_QCXSc8kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lookup_1gram = get_map(nGram=1)\n",
        "# lookup_1gram.values()"
      ],
      "metadata": {
        "id": "-e7Y29YgaN-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lookup_2gram = get_map(nGram=2)\n",
        "# lookup_2gram.keys()\n",
        "# lookup_2gram.values()"
      ],
      "metadata": {
        "id": "r6ac2EfBcgRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lookup_3gram = get_map(nGram=3)"
      ],
      "metadata": {
        "id": "YBxdWalCZZEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write it out to a JSON file that can be loaded as a python dict in the search function\n",
        "with open('json_files/lookup_1gram.json', 'w') as lookup_json:\n",
        "\tjson.dump(lookup_1gram, lookup_json, indent=4)\n",
        "\n",
        "with open('json_files/lookup_2gram.json', 'w') as lookup_json:\n",
        "\tjson.dump(lookup_2gram, lookup_json, indent=4)\n",
        " \n",
        " with open('json_files/lookup_3gram.json', 'w') as lookup_json:\n",
        "\tjson.dump(lookup_2gram, lookup_json, indent=4)"
      ],
      "metadata": {
        "id": "Jhfgq_cTZ3tl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}